{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "\n",
    "* Understand the data types supported by tensors\n",
    "* Type conversion\n",
    "* Importance of device and grad while creating a tensor \n",
    "\n",
    "# Tensor Data Types \n",
    "\n",
    "Tensors support wide range of data types \n",
    "\n",
    "| **Category**         | **Data Type**                    | **Aliases/Notes**              |\n",
    "|-----------------------|----------------------------------|---------------------------------|\n",
    "| **Floating Point**    | 32-bit floating point           | `torch.float32`, `torch.float` |\n",
    "|                       | 64-bit floating point           | `torch.float64`, `torch.double`|\n",
    "|                       | 16-bit floating point           | `torch.float16`, `torch.half`  |\n",
    "|                       | 16-bit floating point (bfloat16)| `torch.bfloat16`               |\n",
    "|                       | 8-bit floating point (e4m3)     | `torch.float8_e4m3fn` (limited support) |\n",
    "|                       | 8-bit floating point (e5m2)     | `torch.float8_e5m2` (limited support)  |\n",
    "| **Complex**           | 32-bit complex                 | `torch.complex32`, `torch.chalf` |\n",
    "|                       | 64-bit complex                 | `torch.complex64`, `torch.cfloat` |\n",
    "|                       | 128-bit complex                | `torch.complex128`, `torch.cdouble` |\n",
    "| **Unsigned Integer**  | 8-bit integer                  | `torch.uint8`                  |\n",
    "|                       | 16-bit integer                 | `torch.uint16` (limited support) |\n",
    "|                       | 32-bit integer                 | `torch.uint32` (limited support) |\n",
    "|                       | 64-bit integer                 | `torch.uint64` (limited support) |\n",
    "| **Signed Integer**    | 8-bit integer                  | `torch.int8`                   |\n",
    "|                       | 16-bit integer                 | `torch.int16`, `torch.short`   |\n",
    "|                       | 32-bit integer                 | `torch.int32`, `torch.int`     |\n",
    "|                       | 64-bit integer                 | `torch.int64`, `torch.long`    |\n",
    "| **Boolean**           | Boolean                        | `torch.bool`                   |\n",
    "| **Quantized**         | Quantized 8-bit integer (unsigned) | `torch.quint8`             |\n",
    "|                       | Quantized 8-bit integer (signed)   | `torch.qint8`              |\n",
    "|                       | Quantized 32-bit integer (signed)  | `torch.qint32`             |\n",
    "|                       | Quantized 4-bit integer (unsigned) | `torch.quint4x2`           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.device \n",
    "\n",
    "A tensor can be created on a particular device (CPU, GPU). The `device` argument to the tensor constructor specifies where the tensor needs to be created. \n",
    "\n",
    "Device options: CPU, CUDA, MPS \n",
    "\n",
    "Notes:\n",
    "* CUDA is a parallel computing platform and application programming interface model for Nvidia GPUs.\n",
    "* MPS (Metal Performance Shaders) is Apple's framework for Apple Silicon GPUs. \n",
    "* Two tensors need to be on the same device to be operated on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA/GPU available? False\n",
      "Is MPS enabled? True\n",
      "MPS built: True\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "print(f\"Is CUDA/GPU available? {th.cuda.is_available()}\") \n",
    "print(f\"Is MPS enabled? {th.backends.mps.is_available()}\")\n",
    "print(\"MPS built:\", th.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tensors of specific type \n",
    "\n",
    "Note: requires_grad flag specifies if gradient needs to be tracked for a tensor. Generally used for backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type of float_32_vector_on_mps: torch.float32\n",
      "Data Type of float_32_vector_on_cpu: torch.float32\n",
      "Device of float_32_vector_on_mps: mps:0\n",
      "Device of float_32_vector_on_cpu: cpu\n"
     ]
    }
   ],
   "source": [
    "float_32_vector_on_mps = th.tensor([1.0, 2.0, 3.0], \n",
    "                                   dtype=th.float32, # Specify the data type \n",
    "                                   device='mps', # Specify the device \n",
    "                                   requires_grad=False) # Specify if gradients are required\n",
    "\n",
    "float_16_vector_on_mps = th.tensor([1.0, 2.0, 3.0], \n",
    "                                   dtype=th.float16, # Specify the data type \n",
    "                                   device='mps', # Specify the device \n",
    "                                   requires_grad=False) # Specify if gradients are required\n",
    "\n",
    "second_float_32_vector_on_mps = th.tensor([4.0, 5.0, 6.0],\n",
    "                                          dtype=th.float32,\n",
    "                                          device='mps',\n",
    "                                          requires_grad=False)\n",
    "\n",
    "\n",
    "\n",
    "float_32_vector_on_cpu = th.tensor([1.0, 2.0, 3.0],\n",
    "                                      dtype=th.float32,\n",
    "                                      device='cpu',\n",
    "                                      requires_grad=False)\n",
    "\n",
    "print(f'Data Type of float_32_vector_on_mps: {float_32_vector_on_mps.dtype}')\n",
    "print(f'Data Type of float_32_vector_on_cpu: {float_32_vector_on_cpu.dtype}')\n",
    "print(f'Device of float_32_vector_on_mps: {float_32_vector_on_mps.device}')\n",
    "print(f'Device of float_32_vector_on_cpu: {float_32_vector_on_cpu.device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 9.], device='mps:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_vector_on_mps + second_float_32_vector_on_mps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Tensors on different devices cannot be operated on together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!\n"
     ]
    }
   ],
   "source": [
    "# Tensor on MPS cannot be added to a tensor on CPU\n",
    "try:\n",
    "    float_32_vector_on_mps + float_32_vector_on_cpu\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Attributres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Device of tensor: mps:0\n",
      "Data Type of tensor: torch.float32\n",
      "Gradient of tensor: False\n",
      "Shape of tensor: torch.Size([3, 3])\n",
      "Is tensor on MPS: True\n",
      "Is tensor on CPU: False\n",
      "Is tensor on CUDA: False\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "Device of tensor: cpu\n",
      "Data Type of tensor: torch.float32\n",
      "Gradient of tensor: True\n",
      "Shape of tensor: torch.Size([3, 3])\n",
      "Is tensor on MPS: False\n",
      "Is tensor on CPU: True\n",
      "Is tensor on CUDA: False\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_device_properties(tensor):\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f'Device of tensor: {tensor.device}')\n",
    "    print(f'Data Type of tensor: {tensor.dtype}')\n",
    "    print(f'Gradient of tensor: {tensor.requires_grad}')\n",
    "    print(f'Shape of tensor: {tensor.shape}')\n",
    "    print(f'Is tensor on MPS: {tensor.is_mps}')\n",
    "    print(f'Is tensor on CPU: {tensor.is_cpu}')\n",
    "    print(f'Is tensor on CUDA: {tensor.is_cuda}')\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "print_device_properties(th.randn(3, 3, device='mps'))\n",
    "print_device_properties(th.randn(3, 3, device='cpu', requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor type conversion\n",
    "\n",
    "The `.to` method can be used to convert the type of a tensor. It can also be used to move the tensor from one device to another.\n",
    "\n",
    "Tensors also have `.float()`, `.bool()` methods for type conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type of float_32_vector_on_mps and f16: torch.float16 and torch.float32\n",
      "Device of float_32_vector_on_mps and f32_on_mps: mps:0 and mps:0\n",
      "tensor([2., 4., 6.], device='mps:0')\n",
      "Boolean value of float_32_vector_on_mps: tensor([True, True, True], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "f16 = float_32_vector_on_mps.to(dtype=th.float16) # Convert to float16\n",
    "print(f'Data Type of float_32_vector_on_mps and f16: {f16.dtype} and {float_32_vector_on_mps.dtype}')\n",
    "\n",
    "f32_on_mps = float_32_vector_on_cpu.to(device='mps') # Move to MPS\n",
    "print(f'Device of float_32_vector_on_mps and f32_on_mps: {f32_on_mps.device} and {f32_on_mps.device}')\n",
    "\n",
    "# After moving float_32_vector_on_cpu to MPS, it can be added to float_32_vector_on_mps\n",
    "print(float_32_vector_on_mps + f32_on_mps)\n",
    "\n",
    "# Type converting using .bool()\n",
    "print(f'Boolean value of float_32_vector_on_mps: {float_32_vector_on_mps.bool()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
